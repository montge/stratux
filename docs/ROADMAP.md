# Stratux Development Roadmap

**Document Version**: 1.0
**Last Updated**: 2025-10-13
**Status**: Draft

---

## Executive Summary

This roadmap outlines the path to achieving DO-278A SAL-3 compliance, 80% test coverage, and enhanced security for the Stratux system over the next 12 months.

**Current State** (Updated 2025-10-13):
- âœ… Mature, functional codebase
- âœ… CI/CD with GitHub Actions (build automation complete)
- âœ… Formal requirements documentation (101 requirements)
- âœ… High-Level Design with Mermaid diagrams
- âœ… DO-278A process documents (SDP, SQAP, CMP)
- âœ… Official standards verified and referenced
- âš ï¸ Limited test coverage (<5%)
- âŒ No unit test infrastructure yet

**Target State** (12 months):
- âœ… DO-278A SAL-3 compliant
- âœ… 80% test coverage
- âœ… Formal requirements and verification
- âœ… Enhanced security posture
- âœ… Continuous compliance infrastructure

---

## Phase 1: Foundation (Months 1-3)

### 1.1 Requirements Engineering (Months 1-2)

**Goal**: Establish formal requirements baseline

**Tasks**:
- [x] Create System Requirements Specification (SRS) - âœ… COMPLETE
- [x] Verify requirements against official standards (DO-260B, DO-282B, GDL90, EUROCONTROL) - âœ… COMPLETE
- [x] Create High-Level Design (HLD) document - âœ… COMPLETE
- [x] Define acceptance criteria for each requirement - âœ… COMPLETE
- [x] Create requirements management process - âœ… COMPLETE (in SDP)
- [ ] Review SRS with stakeholders - ğŸŸ¡ IN PROGRESS
- [ ] Establish requirements traceability matrix - ğŸ”´ NEXT

**Deliverables**:
- âœ… `docs/REQUIREMENTS.md` (101 requirements with official standard references)
- âœ… `docs/HIGH_LEVEL_DESIGN.md` (Architecture with Mermaid diagrams)
- âœ… `docs/SOFTWARE_DEVELOPMENT_PROCESS.md` (Requirements management process included)
- âœ… `docs/SOFTWARE_QUALITY_ASSURANCE_PLAN.md`
- âœ… `docs/CONFIGURATION_MANAGEMENT_PLAN.md`
- â³ `docs/TRACEABILITY_MATRIX.xlsx` - NEXT PRIORITY

**Success Criteria**:
- All major features documented as requirements
- Requirements reviewed and approved
- Traceability established from requirements to code

---

### 1.2 Test Infrastructure (Month 2-3)

**Goal**: Establish comprehensive testing framework

**Tasks**:
- [ ] Set up Go testing framework
- [ ] Configure code coverage tools (`go test -cover`)
- [ ] Integrate coverage reporting in CI/CD
- [ ] Create test data fixtures and mocks
- [ ] Establish testing standards and guidelines
- [ ] Set up test result tracking

**Deliverables**:
- â³ `.github/workflows/test.yml` - Test automation
- â³ `test/README.md` - Testing guide
- â³ `test/fixtures/` - Test data
- â³ `test/mocks/` - Mock implementations
- â³ Coverage reports in CI/CD

**Success Criteria**:
- All tests run automatically on every commit
- Coverage reports generated and published
- Zero-friction test execution for developers

**Implementation**:
```bash
# Enable coverage in CI
go test -v -coverprofile=coverage.out -covermode=atomic ./...
go tool cover -html=coverage.out -o coverage.html

# Upload to codecov or coveralls
bash <(curl -s https://codecov.io/bash)
```

---

## Phase 2: Core Testing (Months 3-6)

### 2.1 Safety-Critical Module Testing (Months 3-4)

**Goal**: 80% coverage of safety-critical functions

**Priority Modules** (in order):
1. **Traffic Fusion** (`main/traffic.go`)
   - Target: 90% coverage
   - Critical: Ownship detection, alerting, position extrapolation

2. **GPS Processing** (`main/gps.go`)
   - Target: 85% coverage
   - Critical: Position validation, accuracy checking

3. **GDL90 Generation** (`main/gen_gdl90.go`)
   - Target: 85% coverage
   - Critical: Message format, CRC, traffic/ownship reports

4. **ADS-B Decoding** (`main/sdr.go`)
   - Target: 80% coverage
   - Critical: Message validation, CRC checking

5. **AHRS Processing** (`main/sensors.go`)
   - Target: 75% coverage
   - Critical: Attitude calculation, sensor validation

**Tasks**:
- [ ] Write unit tests for traffic fusion logic
- [ ] Write unit tests for GPS validation
- [ ] Write unit tests for GDL90 message generation
- [ ] Write unit tests for ADS-B/UAT decoding
- [ ] Write unit tests for AHRS calculations

**Success Criteria**:
- Safety-critical modules achieve target coverage
- All critical functions have test cases
- Tests cover normal, boundary, and error conditions

---

### 2.2 Integration Testing (Months 4-5)

**Goal**: Verify subsystem interactions

**Test Scenarios**:
1. **End-to-End Data Flow**
   - RF Input â†’ Processing â†’ GDL90 Output
   - GPS Input â†’ Position â†’ GDL90 Ownship
   - Multiple traffic sources â†’ Fusion â†’ Output

2. **Error Handling**
   - GPS loss â†’ Graceful degradation
   - SDR failure â†’ Reconnection
   - Invalid messages â†’ Rejection

3. **Performance**
   - 500 msg/sec traffic load
   - Multiple simultaneous clients
   - 8-hour continuous operation

**Tasks**:
- [ ] Create integration test harness
- [ ] Develop test data sets (captured traffic)
- [ ] Implement replay-based regression tests
- [ ] Create performance test suite
- [ ] Document test procedures

**Deliverables**:
- â³ `test/integration/` - Integration tests
- â³ `test/testdata/` - Test data sets
- â³ `docs/TEST_PROCEDURES.md` - Test execution guide

**Success Criteria**:
- All major data paths tested
- Error handling verified
- Performance requirements validated

---

### 2.3 Remaining Module Coverage (Months 5-6)

**Goal**: 80% overall code coverage

**Remaining Modules**:
- Network management (`main/network.go`, `main/clientconnection.go`)
- Weather processing (`main/weather.go`)
- OGN processing (`main/ogn.go`)
- Configuration management (`main/managementinterface.go`)
- Logging (`main/datalog.go`)
- Utilities (`common/`)

**Tasks**:
- [ ] Write unit tests for network management
- [ ] Write unit tests for weather processing
- [ ] Write unit tests for OGN/FLARM
- [ ] Write unit tests for configuration
- [ ] Write unit tests for logging
- [ ] Write unit tests for utility functions

**Success Criteria**:
- Overall code coverage â‰¥ 80%
- All modules have basic test coverage
- Coverage gaps identified and documented

---

## Phase 3: Verification & Validation (Months 6-9)

### 3.1 Requirements Verification (Months 6-7)

**Goal**: Verify all requirements have tests

**Tasks**:
- [ ] Map each requirement to test cases
- [ ] Identify untested requirements
- [ ] Create tests for gaps
- [ ] Document test results
- [ ] Create verification report

**Deliverables**:
- â³ `docs/VERIFICATION_MATRIX.xlsx` - Requirementâ†’Test mapping
- â³ `docs/VERIFICATION_REPORT.md` - Test results summary

**Success Criteria**:
- Every requirement has â‰¥1 test case
- All tests passing
- Traceability complete

---

### 3.2 System Testing (Months 7-8)

**Goal**: Validate system-level requirements

**Test Types**:
1. **Functional Testing**
   - All features work as specified
   - User workflows validated
   - EFB compatibility verified

2. **Non-Functional Testing**
   - Performance benchmarks
   - Reliability (MTBF)
   - Resource usage (CPU, memory, disk)

3. **Field Testing**
   - Real aircraft installation
   - Live traffic reception
   - Multiple EFB apps
   - Various hardware configurations

**Tasks**:
- [ ] Develop system test plan
- [ ] Execute functional test cases
- [ ] Execute performance tests
- [ ] Conduct field testing (alpha users)
- [ ] Document results and issues

**Success Criteria**:
- System meets all functional requirements
- Performance requirements validated
- No critical defects in field testing

---

### 3.3 Security Hardening (Month 8-9)

**Goal**: Enhanced security posture

**Security Improvements**:
1. **Authentication**
   - Web UI login (optional)
   - HTTP Basic Auth or token-based

2. **Input Validation**
   - All user inputs sanitized
   - SQL injection prevention
   - XSS prevention

3. **Secure Updates**
   - Cryptographic signature verification
   - Update integrity checking

4. **Credential Protection**
   - WiFi passwords encrypted
   - Secure storage of sensitive data

**Tasks**:
- [ ] Implement web authentication
- [ ] Audit all user inputs
- [ ] Implement update signing
- [ ] Encrypt stored credentials
- [ ] Conduct security testing (penetration test)

**Success Criteria**:
- Security requirements met
- No high-severity vulnerabilities
- Third-party security audit passed

---

## Phase 4: Compliance & Documentation (Months 9-12)

### 4.1 DO-278A Compliance Documentation (Months 9-10)

**Goal**: Complete DO-278A SAL-3 compliance package

**Documents Required**:
- [x] Software Accomplishment Summary (SAS) - âœ… COMPLETE (`docs/DO-278A-ANALYSIS.md`)
- [x] Software Requirements Specification (SRS) - âœ… COMPLETE (`docs/REQUIREMENTS.md`)
- [x] Software Design Description (SDD) - âœ… COMPLETE (`docs/HIGH_LEVEL_DESIGN.md`)
- [x] Software Development Plan (SDP) - âœ… COMPLETE (`docs/SOFTWARE_DEVELOPMENT_PROCESS.md`)
- [x] Software Configuration Management Plan (SCMP) - âœ… COMPLETE (`docs/CONFIGURATION_MANAGEMENT_PLAN.md`)
- [x] Software Quality Assurance Plan (SQAP) - âœ… COMPLETE (`docs/SOFTWARE_QUALITY_ASSURANCE_PLAN.md`)
- [ ] Software Verification Plan (SVP) - ğŸ”´ TODO (`docs/TEST_PROCEDURES.md`)
- [ ] Software Verification Results (SVR) - ğŸ”´ TODO (generated from test execution)
- [ ] Software Configuration Index (SCI) - ğŸŸ¡ PARTIAL (Git tags, need formal index)
- [ ] Software Quality Assurance Records - ğŸ”´ TODO (audit logs, review records)

**Tasks**:
- [x] Complete High-Level Design document - âœ… COMPLETE
- [x] Create Software Development Plan - âœ… COMPLETE
- [x] Create Configuration Management Plan - âœ… COMPLETE
- [x] Create Quality Assurance Plan - âœ… COMPLETE
- [ ] Create Software Verification Plan - ğŸ”´ NEXT
- [ ] Document all verification results - ğŸ”´ Pending test execution
- [ ] Create configuration index - ğŸŸ¡ In progress (Git tracking)
- [ ] Compile QA records - ğŸ”´ Ongoing
- [ ] Review for completeness - ğŸŸ¡ Quarterly reviews scheduled

**Success Criteria**:
- All DO-278A required documents complete
- Documents reviewed and approved
- Ready for audit

---

### 4.2 Third-Party Audit (Month 11)

**Goal**: Independent verification of compliance

**Audit Scope**:
- DO-278A SAL-3 compliance
- Requirements traceability
- Test coverage adequacy
- Documentation completeness
- Security posture

**Tasks**:
- [ ] Select audit firm
- [ ] Prepare for audit
- [ ] Conduct audit
- [ ] Address findings
- [ ] Obtain certification

**Success Criteria**:
- Audit completed with no major findings
- Minor findings addressed
- Compliance certificate issued

---

### 4.3 Continuous Compliance Infrastructure (Month 12)

**Goal**: Sustain compliance as system evolves

**Infrastructure**:
1. **Automated Testing**
   - Unit tests run on every commit
   - Integration tests run nightly
   - System tests run weekly

2. **Continuous Coverage**
   - Coverage tracked in CI/CD
   - Coverage regression blocked
   - Coverage reports published

3. **Requirements Management**
   - New features require requirements
   - Requirements reviewed before implementation
   - Traceability maintained

4. **Change Control**
   - All changes reviewed
   - Impact analysis performed
   - Verification updated

**Tasks**:
- [ ] Enhance CI/CD with all test types
- [ ] Implement coverage gates
- [ ] Create requirements workflow
- [ ] Document change control process
- [ ] Train team on processes

**Success Criteria**:
- All compliance checks automated
- Coverage maintained above 80%
- Requirements up-to-date
- Change control followed

---

## Milestones

| Milestone | Target Date | Status | Completion |
|-----------|-------------|---------|------------|
| **M1**: Requirements Documented | Month 2 | ğŸŸ¢ COMPLETE | 2025-10-13 |
| **M1a**: Process Documents Created | Month 2 | ğŸŸ¢ COMPLETE | 2025-10-13 |
| **M1b**: Design Documented | Month 2 | ğŸŸ¢ COMPLETE | 2025-10-13 |
| **M2**: Test Infrastructure Ready | Month 3 | ğŸŸ¡ IN PROGRESS | Starting now |
| **M3**: 50% Code Coverage | Month 4 | ğŸ”´ Not Started | - |
| **M4**: Safety-Critical Modules 80%+ | Month 5 | ğŸ”´ Not Started | - |
| **M5**: 80% Overall Coverage | Month 6 | ğŸ”´ Not Started | - |
| **M6**: All Requirements Verified | Month 7 | ğŸ”´ Not Started | - |
| **M7**: System Testing Complete | Month 8 | ğŸ”´ Not Started | - |
| **M8**: Security Hardened | Month 9 | ğŸ”´ Not Started | - |
| **M9**: Documentation Complete | Month 10 | ğŸŸ¡ AHEAD OF SCHEDULE | Core docs done |
| **M10**: Third-Party Audit | Month 11 | ğŸ”´ Not Started | - |
| **M11**: Continuous Compliance | Month 12 | ğŸ”´ Not Started | - |

---

## Resource Requirements

### 4.1 Personnel

**Recommended Team**:
- **Requirements Engineer**: 0.5 FTE (Months 1-3)
- **Test Engineer**: 1.0 FTE (Months 2-9)
- **Software Developer**: 0.5 FTE (Months 3-9, test development)
- **QA Engineer**: 0.5 FTE (Months 6-12)
- **Security Engineer**: 0.5 FTE (Months 8-9)
- **Technical Writer**: 0.25 FTE (Months 9-10)

**Total Effort**: ~3.5 FTE-years

### 4.2 Budget Estimate

| Category | Cost Estimate |
|----------|--------------|
| Personnel (3.5 FTE-years @ $120K/yr) | $420,000 |
| Tools and Infrastructure | $10,000 |
| Third-Party Audit | $25,000 |
| Hardware for Testing | $5,000 |
| **TOTAL** | **$460,000** |

*Note: Costs can be significantly reduced using open-source community contributions*

---

## Success Metrics

### 5.1 Quantitative Metrics

- **Code Coverage**: â‰¥ 80% statement, â‰¥ 70% decision
- **Requirements Coverage**: 100% (all requirements have tests)
- **Test Pass Rate**: â‰¥ 99%
- **Defect Density**: < 0.5 defects/KLOC
- **Build Success Rate**: â‰¥ 95%

### 5.2 Qualitative Metrics

- DO-278A SAL-3 compliance achieved
- Third-party audit passed
- Security vulnerabilities addressed
- Documentation complete and approved
- Community feedback positive

---

## Risks and Mitigation

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Insufficient resources | MEDIUM | HIGH | Leverage open-source community |
| Schedule slip | MEDIUM | MEDIUM | Agile approach, prioritize critical items |
| Scope creep | MEDIUM | MEDIUM | Strict change control |
| Test infrastructure complexity | LOW | MEDIUM | Use proven tools (Go testing) |
| Third-party audit fails | LOW | HIGH | Internal audits before official |
| Team attrition | LOW | HIGH | Documentation, knowledge sharing |

---

## Dependencies

### 6.1 External Dependencies

- GitHub Actions availability (CI/CD)
- Go toolchain updates
- RTL-SDR driver availability
- Raspberry Pi hardware availability

### 6.2 Internal Dependencies

- Community support for testing
- Access to test hardware
- Subject matter expert availability for reviews

---

## Continuous Improvement

This roadmap will be reviewed and updated:
- **Monthly**: Progress review, adjust as needed
- **Quarterly**: Major milestone assessment
- **Annually**: Full roadmap refresh

**Roadmap Owner**: TBD
**Review Board**: TBD

---

## Communication Plan

### 7.1 Stakeholder Updates

- **Monthly**: Progress report to community
- **Quarterly**: Milestone achievements announced
- **Ad-hoc**: Major decisions communicated

### 7.2 Documentation

- Roadmap published on GitHub
- Progress tracked in GitHub Projects
- Milestones visible in GitHub Releases

---

## Conclusion

This roadmap provides a clear path from the current state to DO-278A SAL-3 compliance with 80% test coverage over 12 months. While ambitious, it is achievable with focused effort and community support.

**Completed This Session** (2025-10-13):
1. âœ… Created `REQUIREMENTS.md` with 101 requirements
2. âœ… Created `HIGH_LEVEL_DESIGN.md` with Mermaid diagrams
3. âœ… Created `SOFTWARE_DEVELOPMENT_PROCESS.md`
4. âœ… Created `SOFTWARE_QUALITY_ASSURANCE_PLAN.md`
5. âœ… Created `CONFIGURATION_MANAGEMENT_PLAN.md`
6. âœ… Verified requirements against official standards (DO-260B, DO-282B, GDL90, EUROCONTROL)
7. âœ… Documented official sources and references

**Immediate Next Steps** (Phase 1.2 - Test Infrastructure):
1. Set up basic Go test framework (test structure, naming conventions)
2. Configure GitHub Actions for test execution with coverage
3. Create test fixtures and mock implementations
4. Write first 10 unit tests for traffic.go (ownship detection, extrapolation)
5. Establish requirements traceability matrix spreadsheet
6. Create `docs/TEST_PROCEDURES.md` (Software Verification Plan)

**Let's build a world-class, verified aviation system together!**

---

**END OF ROADMAP**
